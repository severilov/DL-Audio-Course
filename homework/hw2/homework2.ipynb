{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhkTsE_rVd-n"
   },
   "source": [
    "# Homework #2: train a CTC speech recognition model\n",
    "\n",
    "In lecture you have examined the basics of speech recognition and covered the Connectionist Temporal Classification (CTC) model in detail. You are now ready to train your first \"adult\" speech recognition system!\n",
    "\n",
    "In seminar 2 you implemented the CTC forward and backward algorithms in order to calculate the CTC loss and study the diffusion of probability in a CTC trellis. Also you implemented a greedy decoder and a prefix beam-search decoder\n",
    "\n",
    "In this homework you will implement and train a CTC speech recognition model on a subset of the LibriSpeech corpus. This task will involve:\n",
    "\n",
    "- Creating a dataloader\n",
    "- Implementing a and training a Neural Network for CTC\n",
    "  * DNN\n",
    "  * LSTM\n",
    "  * BiLSTM\n",
    "- Comparing the Properties of these models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qg4JIKs4EYtQ"
   },
   "source": [
    "# Setup - Install package, download files, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Gk1mFIiwve6R"
   },
   "outputs": [],
   "source": [
    "# uncomment if needed. If you run the notebook in Colab, all these libraries are pre-installed\n",
    "# !pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install numpy==1.17.5 matplotlib==3.3.3 tqdm==4.54.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VwuUeuXXBDPl",
    "outputId": "dab33b67-9dea-4a62-a26d-e4573551c73e"
   },
   "outputs": [],
   "source": [
    "# !pip install arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HN6poM45AAXH"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# !pip install wandb -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IM01xHP7Y0Hi"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import string\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, TypeVar, Optional, Callable, Iterable\n",
    "\n",
    "import arpa\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import wandb\n",
    "from matplotlib.colors import LogNorm\n",
    "from torch import optim\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "t9lYheNjY0Hn"
   },
   "outputs": [],
   "source": [
    "import utils as utils   # Change relative path if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLRub0wZY0Hs"
   },
   "source": [
    "# Seminar 2 recap: CTC Forward-Backward Algorithm + Soft alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nu4ISUMGt-J"
   },
   "source": [
    "## CTC Forward Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HgRpr3bjY0Hv"
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "BLANK_SYMBOL = \"_\"\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Maps characters to integers and vice versa\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.char_map = {}\n",
    "        self.index_map = {}\n",
    "        for i, ch in enumerate([\"'\", \" \"] + list(string.ascii_lowercase) + [BLANK_SYMBOL]):\n",
    "            self.char_map[ch] = i\n",
    "            self.index_map[i] = ch\n",
    "        \n",
    "    def text_to_indices(self, text: str) -> List[int]:\n",
    "        return [self.char_map[ch] for ch in text]\n",
    "\n",
    "    def indices_to_text(self, labels: List[int]) -> str:                                                                                                                                                                                                                                 \n",
    "        return \"\".join([self.index_map[i] for i in labels])\n",
    "    \n",
    "    def get_symbol_index(self, sym: str) -> int:\n",
    "        return self.char_map[sym]\n",
    "    \n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "NEG_INF = -float(\"inf\")\n",
    "\n",
    "\n",
    "def logsumexp(*args) -> float:\n",
    "    \"\"\"\n",
    "    Log-sum-exp trick for log-domain calculations\n",
    "    See for details: https://en.wikipedia.org/wiki/LogSumExp\n",
    "    \"\"\"\n",
    "    if all(a == NEG_INF for a in args):\n",
    "        return NEG_INF\n",
    "    a_max = max(args)\n",
    "    lsp = math.log(sum(math.exp(a - a_max) for a in args))\n",
    "    return a_max + lsp\n",
    "\n",
    "\n",
    "def modify_sequence(sequence: List[int], blank_idx: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Modifies sequence which with START, END blanks and between each character\n",
    "    \"\"\"\n",
    "    modified_sequence = []\n",
    "    \n",
    "    for idx in sequence:\n",
    "        modified_sequence += [blank_idx, idx]\n",
    "        \n",
    "    modified_sequence.append(blank_idx)\n",
    "    return modified_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FSU94H8-xJMw"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "\n",
    "def forward_algorithm(sequence: List[int], matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param sequence: a string converted to an index array by Tokenizer\n",
    "    :param matrix: A matrix of shape (K, T) with probability distributions over phonemes at each moment of time.\n",
    "    :return: the result of the forward pass of shape (2 * len(sequence) + 1, T)\n",
    "    \"\"\"\n",
    "    # Turn probs into log-probs\n",
    "    matrix = np.log(matrix)\n",
    "    \n",
    "    blank = tokenizer.get_symbol_index(BLANK_SYMBOL)\n",
    "    mod_sequence = modify_sequence(sequence, blank)\n",
    "\n",
    "    # Initialze\n",
    "    # (2L + 1) x T \n",
    "    alphas = np.full([len(mod_sequence), matrix.shape[1]], NEG_INF)\n",
    "\n",
    "    for t in range(matrix.shape[1]):\n",
    "        for s in range(len(mod_sequence)):\n",
    "            # First Step\n",
    "            ch = mod_sequence[s]\n",
    "            if t == 0:\n",
    "                if s != 0 and s != 1:\n",
    "                    alphas[s][t] = NEG_INF\n",
    "                else:\n",
    "                    alphas[s][t] = matrix[ch][t]\n",
    "                \n",
    "            # Upper diagonal zeros\n",
    "            elif s < alphas.shape[0] - 2 * (alphas.shape[1]-t)-1:# CONDITION\n",
    "                alphas[s][t] = NEG_INF\n",
    "            else:\n",
    "                # Need to do this stabily\n",
    "                if s == 0:\n",
    "                    alphas[s][t] = alphas[s][t-1] + matrix[ch][t]\n",
    "                elif s == 1:\n",
    "                    alphas[s][t] = logsumexp(alphas[s][t-1], alphas[s-1][t-1]) + matrix[ch][t]\n",
    "                else:\n",
    "                    if ch == blank or ch == mod_sequence[s-2]:\n",
    "                        alphas[s][t] = logsumexp(alphas[s][t-1], alphas[s-1][t-1]) + matrix[ch][t]\n",
    "                    else:\n",
    "                        alphas[s][t] = logsumexp(alphas[s][t-1], alphas[s-1][t-1], alphas[s-2][t-1]) + matrix[ch][t]\n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOSUt5iox8B6"
   },
   "source": [
    "## The CTC Backward Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hTaA3O8nHArw"
   },
   "outputs": [],
   "source": [
    "def backward_algorithm(sequence: List[int], matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param sequence: a string converted to an index array by Tokenizer\n",
    "    :param matrix: A matrix of shape (K, T) with probability distributions over phonemes at each moment of time.\n",
    "    :return: the result of the backward pass of shape (2 * len(sequence) + 1, T)\n",
    "    \"\"\"\n",
    "    matrix = np.log(matrix)\n",
    "    blank = tokenizer.get_symbol_index(BLANK_SYMBOL)\n",
    "    mod_sequence = modify_sequence(sequence, blank)\n",
    "    betas = np.full([len(mod_sequence), matrix.shape[1]], NEG_INF)\n",
    "\n",
    "    for t in reversed(range(matrix.shape[1])):\n",
    "        for s in reversed(range(len(mod_sequence))):\n",
    "            # First Step\n",
    "            ch = mod_sequence[s]\n",
    "            if t == matrix.shape[1] - 1:\n",
    "                if s == betas.shape[0]-1 or s == betas.shape[0]-2:\n",
    "                    betas[s][t] = 0\n",
    "\n",
    "            # Lower Diagonal Zeros\n",
    "            elif s > 2 * t + 1:# CONDITION\n",
    "                betas[s][t] = NEG_INF\n",
    "            else:\n",
    "                if s == len(mod_sequence) - 1:\n",
    "                    betas[s][t] = betas[s][t+1] + matrix[ch][t]\n",
    "                elif s == len(mod_sequence) - 2:\n",
    "                    betas[s][t] = logsumexp(betas[s][t+1], betas[s+1][t+1]) + matrix[ch][t]\n",
    "                else:\n",
    "                    if ch == blank or ch == mod_sequence[s + 2]:\n",
    "                            betas[s][t] = logsumexp(betas[s][t+1], betas[s+1][t+1]) + matrix[ch][t]\n",
    "                    else:                \n",
    "                        betas[s][t] = logsumexp(betas[s][t+1], betas[s+1][t+1], betas[s+2][t+1]) + matrix[ch][t]\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TULzSmkZxgEn"
   },
   "source": [
    "## Soft-Alignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "r9Xo-1tvHEBX"
   },
   "outputs": [],
   "source": [
    "def soft_alignment(labels_indices: List[int], matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns the alignment coefficients for the input sequence\n",
    "    \"\"\"\n",
    "    alphas = forward_algorithm(labels_indices, matrix)\n",
    "    betas = backward_algorithm(labels_indices, matrix)\n",
    "\n",
    "    # Move from log space back to prob space\n",
    "    align = np.exp(alphas + betas)\n",
    "\n",
    "    # Normalize Alignment\n",
    "    align = align / np.sum(align, axis=0)\n",
    "\n",
    "    return align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQNJqD8mx_0i"
   },
   "source": [
    "## Greedy Best-Path Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qQhIqk30x4nv"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "def greedy_decoder(output: torch.Tensor, labels: List[torch.Tensor], \n",
    "                   label_lengths: List[int], collapse_repeated: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    :param output: torch.Tensor of Probs or Log-Probs of shape [batch, time, classes]\n",
    "    :param labels: list of label indices converted to torch.Tensors\n",
    "    :param label_lengths: list of label lengths (without padding)\n",
    "    :param collapse_repeated: whether the repeated characters should be deduplicated\n",
    "    :return: the result of the decoding and the target sequence\n",
    "    \"\"\"\n",
    "    blank_label = tokenizer.get_symbol_index(BLANK_SYMBOL)\n",
    "\n",
    "    # Get max classes\n",
    "    ########################\n",
    "    arg_maxes = output.argmax(dim=-1)\n",
    "    ########################\n",
    "\n",
    "    decodes = []\n",
    "    targets = []\n",
    "\n",
    "    # For targets and decodes remove repeats and blanks\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        true_labels = labels[i][:label_lengths[i]].tolist()\n",
    "        targets.append(tokenizer.indices_to_text(true_labels))\n",
    "\n",
    "        # Remove repeats, then remove blanks\n",
    "        for j, index in enumerate(args):\n",
    "            ########################\n",
    "            if j != 0:\n",
    "                if index == args[j-1]:\n",
    "                    continue\n",
    "            decode.append(int(index.cpu().detach()))    \n",
    "            ########################\n",
    "        ####\n",
    "        decode = [x for x in decode if x != blank_label]\n",
    "        ######\n",
    "        \n",
    "        decodes.append(tokenizer.indices_to_text(decode))\n",
    "    return decodes, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oe_g0pXw21Tn"
   },
   "source": [
    "## Prefix Decoding With LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "AqYtZjJhY0H0"
   },
   "outputs": [],
   "source": [
    "LanguageModel = TypeVar(\"LanguageModel\")\n",
    "# Helper function\n",
    "\n",
    "class Beam:\n",
    "    def __init__(self, beam_size: int) -> None:\n",
    "        self.beam_size = beam_size\n",
    "        \n",
    "        fn = lambda : (NEG_INF, NEG_INF)\n",
    "        \n",
    "        # Store probs key - prefix, value - p_blank, p_not_blank for ? step\n",
    "        self.candidates = defaultdict(fn)\n",
    "        \n",
    "        # Store sorted by cumulative probability self.candidates\n",
    "        self.top_candidates_list = [\n",
    "            (\n",
    "                tuple(), \n",
    "                (0.0, NEG_INF) # log(p_blank) = 0, log(p_not_blank) = -inf\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "    def get_probs_for_prefix(self, prefix: Tuple[int]) -> Tuple[float, float]:\n",
    "        p_blank, p_not_blank = self.candidates[prefix]\n",
    "        return p_blank, p_not_blank\n",
    "        \n",
    "    def update_probs_for_prefix(self, prefix: Tuple[int], next_p_blank: float, next_p_not_blank: float) -> None:\n",
    "        self.candidates[prefix] = (next_p_blank, next_p_not_blank)\n",
    "        \n",
    "    def update_top_candidates_list(self) -> None:\n",
    "        top_candidates = sorted(\n",
    "            self.candidates.items(), \n",
    "            key=lambda x: logsumexp(*x[1]), \n",
    "            reverse=True\n",
    "        )\n",
    "        self.top_candidates_list = top_candidates[:self.beam_size]\n",
    "        \n",
    "\n",
    "def calculate_probability_score_with_lm(lm: LanguageModel, prefix: str) -> float:\n",
    "    text = tokenizer.indices_to_text(prefix).upper().strip()    # Use upper case for LM and remove the trailing space\n",
    "    lm_prob = lm.log_p(text)             \n",
    "    score = lm_prob / np.log10(np.e)    # Convert to natural log, as ARPA LM uses log10   \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "SJ2pts572m5W"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "\n",
    "def decode(probs: np.ndarray, beam_size: int = 5, lm: Optional[LanguageModel] = None, \n",
    "           prune: float = 1e-5, alpha: float = 0.1, beta: float = 2):\n",
    "    \"\"\"\n",
    "    :param probs: A matrix of shape (T, K) with probability distributions over phonemes at each moment of time.\n",
    "    :param beam_size: the size of beams\n",
    "    :lm: arpa language model\n",
    "    :prune: the minimal probability for a symbol at which it can be added to a prefix\n",
    "    :alpha: the parameter to de-weight the LM probability\n",
    "    :beta: the parameter to up-weight the length correction term\n",
    "    :return: the prefix with the highest sum of probabilites P_blank and P_not_blank\n",
    "    \"\"\"\n",
    "    T, S = probs.shape\n",
    "    probs = np.log(probs)\n",
    "    blank = tokenizer.get_symbol_index(BLANK_SYMBOL)\n",
    "    space = tokenizer.get_symbol_index(\" \")\n",
    "    prune = NEG_INF if prune == 0.0 else np.log(prune)\n",
    "    \n",
    "    beam = Beam(beam_size)\n",
    "    # –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –æ—Å–∏ –≤—Ä–µ–º–µ–Ω–∏\n",
    "    for t in range(T):\n",
    "        next_beam = Beam(beam_size)\n",
    "        \n",
    "        # –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ —Å–∏–º–≤–æ–ª–∞–º\n",
    "        for s in range(S):\n",
    "            p = probs[t, s]\n",
    "            # Prune the vocab - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∏–º–≤–æ–ª, –µ—Å–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ–∫–∞–∑–∞—Ç—å—Å—è –≤ –Ω–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–∞ –Ω–∞ t-–º —â–∞–≥–µ\n",
    "            if p < prune:   \n",
    "                continue\n",
    "            \n",
    "            # –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –≤–∞—Ä–∏–Ω–∞—Ç–∞–º, –≤ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–µ–º –ø–æ–π—Ç–∏ –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Å–∏–º–≤–æ–ª–∞\n",
    "            # –°–Ω–∞—á–∞–ª–∞ –∏–¥—É—Ç –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–µ –ø–æ —Å—É–º–º–µ log(p_blank + p_not_blank) –ø—Ä–µ—Ñ–∏–∫—Å—ã\n",
    "            # (p_blank, p_not_blank) - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–º t-1 —à–∞–≥–µ\n",
    "            for prefix, (p_blank, p_not_blank) in beam.top_candidates_list:\n",
    "                # –¢–µ–∫—É—â–∏–π —Å–∏–º–≤–æ–ª - –±–ª–∞–Ω–∫ \n",
    "                if s == blank:\n",
    "                    # –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ —Ç–µ–∫—É—â–µ–º —à–∞–≥–µ\n",
    "                    p_b, p_nb = next_beam.get_probs_for_prefix(prefix)\n",
    "                    next_beam.update_probs_for_prefix(\n",
    "                        prefix=prefix,\n",
    "                        next_p_blank=logsumexp(p_b, p_blank + p, p_not_blank + p),\n",
    "                        next_p_not_blank=p_nb\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                end_t = prefix[-1] if prefix else None\n",
    "                n_prefix = prefix + (s,)\n",
    "                \n",
    "                # –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è —Å–∏–º–≤–æ–ª\n",
    "                if s == end_t:\n",
    "                    # –ü—Ä–µ–¥—ã–¥—É—â–∏–π —Å–∏–º–≤–æ–ª - –±–ª–∞–Ω–∫\n",
    "                    p_b, p_nb = next_beam.get_probs_for_prefix(n_prefix)\n",
    "                    next_beam.update_probs_for_prefix(\n",
    "                        prefix=n_prefix,\n",
    "                        next_p_blank=p_b,\n",
    "                        next_p_not_blank=logsumexp(p_nb, p + p_blank)\n",
    "                    )\n",
    "                    # –ü—Ä–µ–¥—É–¥—â–∏–π —Å–∏–º–≤–æ–ª –Ω–µ –±–ª–∞–Ω–∫\n",
    "                    p_b, p_nb = next_beam.get_probs_for_prefix(prefix)\n",
    "                    next_beam.update_probs_for_prefix(\n",
    "                        prefix=prefix,\n",
    "                        next_p_blank=p_b,\n",
    "                        next_p_not_blank=logsumexp(p_nb, p + p_not_blank)\n",
    "                    )\n",
    "                elif s == space and end_t is not None and lm is not None:\n",
    "                    # –°–∏–º–≤–æ–ª - –ø—Ä–æ–±–µ–ª –∏ –Ω–µ –ø–µ—Ä–≤—ã–π, –Ω—É–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å\n",
    "                    p_b, p_nb = next_beam.get_probs_for_prefix(n_prefix)\n",
    "                    score = calculate_probability_score_with_lm(lm, n_prefix)\n",
    "                    length = len(tokenizer.indices_to_text(prefix))\n",
    "                    \n",
    "                    next_beam.update_probs_for_prefix(\n",
    "                        prefix=n_prefix,         \n",
    "                        next_p_blank=p_b,\n",
    "                        next_p_not_blank=logsumexp(\n",
    "                            p_nb,\n",
    "                            p_blank + p + score * alpha + np.log(length) * beta,\n",
    "                            p_not_blank + p + score * alpha + np.log(length) * beta\n",
    "                        )  \n",
    "                    )\n",
    "                else:\n",
    "                    p_b, p_nb = next_beam.get_probs_for_prefix(n_prefix)\n",
    "                    next_beam.update_probs_for_prefix(\n",
    "                        prefix=n_prefix,\n",
    "                        next_p_blank=p_b,\n",
    "                        next_p_not_blank=logsumexp(p_nb, p_blank + p, p_not_blank + p)\n",
    "                    )\n",
    "\n",
    "        next_beam.update_top_candidates_list()\n",
    "        beam = next_beam\n",
    "\n",
    "    best = beam.top_candidates_list[0]\n",
    "    return best[0], -logsumexp(*best[1])\n",
    "\n",
    "\n",
    "def beam_search_decoder(probs: np.ndarray, labels: List[List[int]], label_lengths: List[int], \n",
    "                        input_lengths: List[int], lm: LanguageModel, beam_size: int = 5,\n",
    "                        prune: float = 1e-3, alpha: float = 0.1, beta: float = 0.1):\n",
    "    probs = probs.cpu().detach().numpy()\n",
    "    decodes, targets = [], []\n",
    "    \n",
    "    for i, prob in enumerate(probs):\n",
    "        targets.append(tokenizer.indices_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "        int_seq, _ = decode(prob[:input_lengths[i]], lm=lm, beam_size=beam_size, prune=prune, alpha=alpha, beta=beta)\n",
    "        decodes.append(tokenizer.indices_to_text(int_seq))\n",
    "        \n",
    "    return decodes, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doD9f6gZ2RXx"
   },
   "source": [
    "# Homework 2 starts here: CTC Speech Recognition System\n",
    "You can do this notebook in google collab, or use other GPU sources\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- (25 points) Train ASR System, WER criterions: 60-50 -- 9 points, 50-40 -- 15 points, 40-35 -- 20 points, <=35 -- 25 points. + 1 bonus point per 1% WER below 30\n",
    "- (2 points) Compare performance of DNN, RNN and BiRNN models in terms of WER, training time and other properties\n",
    "- (3 points) Compare alignments obtained from DNN, RNN and BiRNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55HlLr7W63JJ"
   },
   "source": [
    "## Implementing, training and evaluationg your CTC ASR model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyU-wpIU67yu"
   },
   "source": [
    "### Create a Dataloader\n",
    "\n",
    "The first step is to create a dataloader to download and load and preprocess LibriSpeech acoustic data. \n",
    "\n",
    "The creative options you have at this stage are:\n",
    "\n",
    "* The sample rate and number of mel-bins.\n",
    "* Various forms of data agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "5de35340d6844d229bd8ce7aef425e29",
      "39b18c3bb5e74345af203a673160e75a",
      "6ae4780ddda84918ac85b1b2df8b8d35",
      "f270e58c76234ed9a532ddf73fef56b9",
      "3d20813547484685b82488b48047f8cb",
      "c5f74921d7a241abb0faf420038bba2f",
      "00fad4fd55734855b6c7a6064f5f5539",
      "3446a1862240433986859ff654da072e",
      "fea0998bea934670936c784dd9fc39c9",
      "a5175588d5a24d8ba90d699887e5b8bb",
      "fabcad8fad7d40c5bd07debd55b0a184",
      "d859df92a88742adb725137b0105d40b",
      "5d27efc374ef4ee194630ef570e7711a",
      "57612ae51b3b48c9be2fec9ffc78feae",
      "91a64e7a24044c7cafa46a8fb7674e8d",
      "831bda93ff3c47f7aa656c7d74b35307",
      "c6e704f8a8d94193bab65f6b9fa8b923",
      "4c4bfd62d26b4a028382ee2f57172d4a",
      "22067a7eff2846d980ed8709b5ccf34d",
      "ebc2bb77d43e46d9be161b708f5aa72c",
      "eddb4d0949074c02b2c11bada933e0dc",
      "0aaae5f2d48346d0b628c190a3f303c6"
     ]
    },
    "id": "qaeFHkHT2jqu",
    "outputId": "6bd7dff5-cdd1-4461-d398-3ad5e42e01a7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3514f344fa92401ca9b1ff10d81badef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/5.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54fb04061a1436787cd12969e316a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!L\n",
    "# Download LibriSpeech 100hr training and test data\n",
    "\n",
    "if not os.path.isdir(\"./data\"):\n",
    "    os.makedirs(\"./data\")\n",
    "\n",
    "train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"train-clean-100\", download=True)\n",
    "test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"test-clean\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ZFSPjx7zY0H2"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "# For train you can use SpecAugment data aug here.\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    #Add features transformations here\n",
    "                ########################\n",
    "                # YOUR CODE HERE\n",
    "                ########################\n",
    "    #Can add data augmentation here!\n",
    "                ########################\n",
    "                # YOUR CODE HERE\n",
    "                ########################\n",
    ")\n",
    "\n",
    "test_audio_transforms = nn.Sequential(\n",
    "    #Add feature transformations here\n",
    "                ########################\n",
    "                # YOUR CODE HERE\n",
    "                ########################\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "noWmJGQe67IJ"
   },
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, data_type = 'test') -> None:\n",
    "        super(Collate, self).__init__() \n",
    "\n",
    "        self.data_type = data_type\n",
    "\n",
    "    def __call__(self, data: torchaudio.datasets.librispeech.LIBRISPEECH) -> Tuple[List[torch.Tensor], ...]:\n",
    "        \"\"\"\n",
    "        :param data: is a list of tuples of [features, label], where features has dimensions [n_features, length]\n",
    "        \"returns features, lengths, labels: \n",
    "              features is a Tensor [batchsize, features, max_length]\n",
    "              lengths is a Tensor of lengths [batchsize]\n",
    "              labels is a Tesnor of targets [batchsize]\n",
    "        \"\"\"\n",
    "\n",
    "        spectrograms = []\n",
    "        labels = []\n",
    "        input_lengths = []\n",
    "        label_lengths = []\n",
    "        for (waveform, _, utterance, _, _, _) in data:\n",
    "            if self.data_type == 'train':\n",
    "                spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "            elif self.data_type == 'test':\n",
    "                spec = test_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "            else:\n",
    "                raise Exception('data_type should be train or valid')\n",
    "            spectrograms.append(spec)\n",
    "            label = torch.Tensor(tokenizer.text_to_indices(utterance.lower()))\n",
    "            labels.append(label)\n",
    "            input_lengths.append(spec.shape[0] // 2)\n",
    "            label_lengths.append(len(label))\n",
    "\n",
    "        spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "        labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "        return spectrograms, labels, input_lengths, label_lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPYITlXT7AoG"
   },
   "source": [
    "### Implement a Neural Network Model\n",
    "\n",
    "You should try out a few different model types:\n",
    "- Feed-Forward Model (DNN)\n",
    "- Recurrent Model (GRU or LSTM)\n",
    "- Bidirectional Recurrent Model (bi-GRU or bi-LSTM)\n",
    "- Something different for bonus points\n",
    "\n",
    "Before any of this models you can use convolutional layers, as shown in the example below\n",
    "\n",
    "After your experiments you should write a report with comparison of different models in terms of different features, for example: parameters, training speed, resulting quality, spectrogram properties, and data augmentations. Remember, that for full mark you need to achive good WER \n",
    "\n",
    "WER criterions: 60-50 -- 9 points, 50-40 -- 15 points, 40-35 -- 20 points, <= 35 -- 25 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "c1eUPtQ57EoX"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "\n",
    "# Our model classes are just examples, you can change them as you want\n",
    "\n",
    "# Define model\n",
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization built for CNNs input\"\"\"\n",
    "\n",
    "    def __init__(self, n_feats: int) -> None:\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous()  # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous()  # (batch, channel, feature, time)\n",
    "\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "        except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel: int, stride: int, dropout: float, n_feats: int) -> None:\n",
    "        super(ResidualCNN, self).__init__()\n",
    "\n",
    "        ########################\n",
    "        # YOUR CODE HERE\n",
    "        ########################\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = # YOUR CODE\n",
    "        x += residual\n",
    "        return x  # (batch, channel, feature, time)\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "\n",
    "    def __init__(self, n_cnn_layers: int, n_rnn_layers: int, rnn_dim: int,\n",
    "                 n_feats: int, stride: int = 2, dropout: float = 0.1) -> None:\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        n_feats = n_feats // 2\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3 // 2)  # cnn for extracting heirachal features\n",
    "\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = # YOUR CODE\n",
    "        self.fully_connected = # YOUR CODE\n",
    "      \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2)  # (batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CTCDNN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_cnn_layers: int, n_rnn_layers: int, rnn_dim: int, n_class: int, \n",
    "                 n_feats: int, stride: int = 2, dropout: float = 0.1) -> None:\n",
    "        super(CTCDNN, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = FeatureExtractor(n_cnn_layers, n_rnn_layers, rnn_dim,\n",
    "                 n_feats, stride, dropout)\n",
    "        \n",
    "        ########################\n",
    "        # YOUR CODE HERE\n",
    "        ########################\n",
    "\n",
    "        self.classifier = nn.Linear(rnn_dim, n_class)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, input_lengths: torch.Tensor) -> torch.Tensor:\n",
    "            x = self.feature_extractor(x)\n",
    "            x = self.intermediate_layers(x)\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "\n",
    "class CTCRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_cnn_layers: int, n_rnn_layers: int, rnn_dim: int, n_class: int, \n",
    "                 n_feats: int, stride: int = 2, dropout: float = 0.1) -> None:\n",
    "        super(CTCRNN, self).__init__()\n",
    "\n",
    "        self.feature_extractor = FeatureExtractor(n_cnn_layers, n_rnn_layers, rnn_dim,\n",
    "                        n_feats, stride, dropout)\n",
    "\n",
    "        ########################\n",
    "        # YOUR CODE HERE\n",
    "        ########################\n",
    "\n",
    "\n",
    "        self.classifier = nn.Linear(rnn_dim, n_class)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, input_lengths: torch.Tensor) -> torch.Tensor:\n",
    "            x = self.feature_extractor(x)\n",
    "            x, _ = self.intermediate_layers(x)\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "class CTCBiRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_cnn_layers: int, n_rnn_layers: int, rnn_dim: int, n_class: int, \n",
    "                 n_feats: int, stride: int = 2, dropout: float = 0.1) -> None:\n",
    "        super(CTCBiRNN, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = FeatureExtractor(n_cnn_layers, n_rnn_layers, rnn_dim,\n",
    "                 n_feats, stride, dropout)\n",
    "\n",
    "        ########################\n",
    "        # YOUR CODE HERE\n",
    "        ########################\n",
    "\n",
    "\n",
    "        self.classifier = nn.Linear(2*rnn_dim, n_class)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, input_lengths: torch.Tensor) -> torch.Tensor:\n",
    "            x = self.feature_extractor(x)\n",
    "            x, _ = self.intermediate_layers(x)\n",
    "            x = self.classifier(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmzKVa497Tav"
   },
   "source": [
    "### Training and Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "v_O37QIX3nft"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "\n",
    "def train(model: nn.Module, device: str, train_loader: data.DataLoader, \n",
    "          criterion: nn.Module, optimizer: torch.optim.Optimizer, \n",
    "          scheduler: torch.optim.lr_scheduler, epoch: int) -> None:\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    for batch_idx, _data in enumerate(train_loader):\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(spectrograms, input_lengths)  # (batch, time, n_class)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        output = output.transpose(0, 1)  # (time, batch, n_class)\n",
    "\n",
    "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(spectrograms), data_len,\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "            wandb.log({'loss_train': loss.item()})\n",
    "\n",
    "\n",
    "def test(model: nn.Module, device: str, test_loader: data.DataLoader, \n",
    "         criterion: nn.Module, epoch: int, decode: str = 'Greedy', lm: LanguageModel = None, save_path: str = None) -> None:\n",
    "    print('Beginning eval...')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        for i, _data in enumerate(test_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            \n",
    "            matrix = model(spectrograms, input_lengths)  # (batch, time, n_class)\n",
    "            matrix = F.log_softmax(matrix, dim=2)\n",
    "            probs = F.softmax(matrix,dim=2)\n",
    "            matrix = matrix.transpose(0, 1)  # (time, batch, n_class)\n",
    "                \n",
    "            if i == 3:\n",
    "                np.savetxt(f\"{save_path}_matrix.txt\", probs[0].cpu().numpy())\n",
    "                np.savetxt(f\"{save_path}_labels.txt\", labels[0].cpu().numpy())\n",
    "\n",
    "            loss = criterion(matrix, labels, input_lengths, label_lengths)\n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "            if decode == 'Greedy':\n",
    "                decoded_preds, decoded_targets = greedy_decoder(matrix.transpose(0, 1), labels, label_lengths)\n",
    "            elif decode == 'BeamSearch':\n",
    "                ## THIS IS THE FUNCTION YOU SHOULD IMPLEMENT\n",
    "                decoded_preds, decoded_targets = beam_search_decoder(probs, labels, label_lengths, input_lengths, lm=lm)\n",
    "            for j in range(len(decoded_preds)):\n",
    "                test_cer.append(utils.cer(decoded_targets[j], decoded_preds[j]))\n",
    "                test_wer.append(utils.wer(decoded_targets[j], decoded_preds[j]))\n",
    "\n",
    "    avg_cer = sum(test_cer) / len(test_cer)\n",
    "    avg_wer = sum(test_wer) / len(test_wer)\n",
    "    wandb.log({'loss_test': test_loss, 'avg_cer': avg_cer, 'avg_wer': avg_wer})\n",
    "    print(\n",
    "        'Epoch: {:d}, Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(epoch, test_loss,\n",
    "                                                                                                       avg_cer,\n",
    "                                                                                                       avg_wer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "iO-mKLPt7Xhb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only CPU found! üíª\n"
     ]
    }
   ],
   "source": [
    "#!L\n",
    "torch.manual_seed(7)\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU found! üéâ')\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print('Only CPU found! üíª')\n",
    "    device = 'cpu'\n",
    "\n",
    "verbose=False\n",
    "\n",
    "# Hyperparameters for your model\n",
    "hparams = {\n",
    "    \"n_cnn_layers\": #YOUR CODE\n",
    "    \"n_rnn_layers\": #YOUR CODE\n",
    "    \"rnn_dim\": #YOUR CODE\n",
    "    \"n_class\": 29,\n",
    "    \"n_feats\": #YOUR CODE\n",
    "    \"stride\": #YOUR CODE\n",
    "    \"dropout\": #YOUR CODE\n",
    "    \"learning_rate\":  #YOUR CODE\n",
    "    \"batch_size\":  #YOUR CODE\n",
    "    \"epochs\": #YOUR CODE\n",
    "}\n",
    "\n",
    "train_collate_fn = Collate(data_type='train')\n",
    "test_collate_fn = Collate(data_type='test')\n",
    "\n",
    "# Define Dataloyour training and test data loaders\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} if device == 'cuda' else {}\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=True, collate_fn=train_collate_fn, **kwargs)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=hparams['batch_size'], shuffle=False, collate_fn=test_collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_uhuVgw2XbB",
    "outputId": "5910f7ac-260c-4506-b910-500bc8dcd645"
   },
   "source": [
    "We recommend to use \"Weights & Biases\" for experiment logging. See their [documentation](https://docs.wandb.ai/) for detais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "3cVNk7KkY0H4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/19535739/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/19535739/codes/speech_course/03_ctc/wandb/run-20221206_232757-9kcdkhfr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/oorgien/speech-hw2/runs/9kcdkhfr\" target=\"_blank\">magic-glitter-1</a></strong> to <a href=\"https://wandb.ai/oorgien/speech-hw2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/oorgien/speech-hw2/runs/9kcdkhfr?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x17de0a290>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"hw2-dlaudio\", \n",
    "           group=\"DNN\",\n",
    "           config=hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gB79UDNM8UOZ"
   },
   "source": [
    "## Compare different models: DNN, GRU/LSTM, bi-GRU/bi-LSTM (2 points)\n",
    "\n",
    "Train and discuss differences in the different models. \n",
    "\n",
    "Compare performance of DNN, RNN and BiRNN models in terms of:\n",
    "-  WER / CER \n",
    "-  Training time\n",
    "-  Training stability \n",
    "-  Any other properties?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJZH5l2YY0H5"
   },
   "outputs": [],
   "source": [
    "# Train a non-recurrent model\n",
    "ctc_dnn = CTCDNN(\n",
    "    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout'])\n",
    "ctc_dnn.to(device)\n",
    "\n",
    "\n",
    "optimizer = # YOUR CODE  - SUGGESTED ADAM/ADAMW\n",
    "criterion = nn.CTCLoss(blank=tokenizer.get_symbol_index(BLANK_SYMBOL), reduction='mean')\n",
    "scheduler = # YOUR CODE  - SUGGESTED ONE CYCLE\n",
    "\n",
    "for epoch in tqdm(range(1, hparams['epochs'] + 1)):\n",
    "    train(ctc_dnn, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
    "    utils.save_checkpoint(ctc_dnn, checkpoint_name=f'ctc_dnn_epoch{epoch}.tar')\n",
    "    wandb.save(f'ctc_dnn_epoch{epoch}.tar')\n",
    "    test(ctc_dnn, device, test_loader, criterion, epoch, 'dnn')\n",
    "\n",
    "utils.save_checkpoint(ctc_dnn, checkpoint_name=f'ctc_dnn.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EnjLgyVrWq83"
   },
   "outputs": [],
   "source": [
    "# Train a  recurrent model\n",
    "ctc_rnn = CTCRNN(\n",
    "    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    ").to(device)\n",
    "\n",
    "optimizer = # YOUR CODE  - SUGGESTED ADAM/ADAMW\n",
    "criterion = nn.CTCLoss(blank=tokenizer.get_symbol_index(BLANK_SYMBOL), reduction='mean')\n",
    "scheduler = # YOUR CODE  - SUGGESTED ONE CYCLE\n",
    "\n",
    "for epoch in tqdm(range(1, hparams['epochs'] + 1)):\n",
    "    train(ctc_rnn, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
    "    utils.save_checkpoint(ctc_rnn, checkpoint_name=f'ctc_rnn_epoch{epoch}.tar')\n",
    "    wandb.save(f'ctc_rnn_epoch{epoch}.tar')\n",
    "    test(ctc_rnn, device, test_loader, criterion, epoch, 'rnn')\n",
    "\n",
    "utils.save_checkpoint(ctc_rnn, checkpoint_name=f'ctc_rnn.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4ylbT6eWqx_"
   },
   "outputs": [],
   "source": [
    "# Train a  recurrent model\n",
    "ctc_birnn = CTCBiRNN(\n",
    "    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    ").to(device)\n",
    "\n",
    "optimizer = # YOUR CODE  - SUGGESTED ADAM/ADAMW\n",
    "criterion = nn.CTCLoss(blank=tokenizer.get_symbol_index(BLANK_SYMBOL), reduction='mean')\n",
    "scheduler = # YOUR CODE  - SUGGESTED ONE CYCLE\n",
    "\n",
    "for epoch in tqdm(range(1, hparams['epochs'] + 1)):\n",
    "    train(ctc_birnn, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
    "    utils.save_checkpoint(ctc_birnn, checkpoint_name=f'ctc_birnn_epoch{epoch}.tar')\n",
    "    wandb.save(f'ctc_birnn_epoch{epoch}.tar')\n",
    "    test(ctc_birnn, device, test_loader, criterion, epoch, 'birnn')\n",
    "\n",
    "utils.save_checkpoint(ctc_birnn, checkpoint_name=f'ctc_birnn.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVLB1aH_ljcw"
   },
   "source": [
    "## Compare alignments (3 points)\n",
    "\n",
    "In this section you should compare alignments obtained from different models (DNN / RNN / BiRNN). For example, you can show:\n",
    "\n",
    "- Examples of alignments and their analysis. \n",
    "- Differencies in the properties of alignment distributions over the dataset. \n",
    "- Dynamic of alignments during training (from checkpoints). \n",
    "- Connection between alignments and model loss. \n",
    "- Which models use the most blanks and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lF0rnu1O3kdU"
   },
   "outputs": [],
   "source": [
    "# Some code to get you started.\n",
    "dnn_matrix = np.loadtxt('dnn_matrix.txt').T\n",
    "rnn_matrix = np.loadtxt('rnn_matrix.txt').T\n",
    "birnn_matrix = np.loadtxt('birnn_matrix.txt').T\n",
    "\n",
    "dnn_labels = np.loadtxt('dnn_labels.txt', dtype=np.int32)\n",
    "dnn_labels = dnn_labels[dnn_labels != 0]\n",
    "rnn_labels = np.loadtxt('rnn_labels.txt', dtype=np.int32)\n",
    "rnn_labels = rnn_labels[dnn_labels != 0]\n",
    "birnn_labels = np.loadtxt('birnn_labels.txt', dtype=np.int32)\n",
    "birnn_labels = birnn_labels[birnn_labels != 0]\n",
    "\n",
    "dnn_align = soft_alignment(dnn_labels, dnn_matrix)\n",
    "rnn_align = soft_alignment(rnn_labels, rnn_matrix)\n",
    "birnn_align = soft_alignment(birnn_labels, birnn_matrix)\n",
    "\n",
    "f, ax = plt.subplots(3, 2, dpi=75, figsize=(15, 15))\n",
    "\n",
    "\n",
    "im = ax[0,0].imshow(dnn_align, aspect='auto', interpolation='nearest')\n",
    "ax[0,0].set_title(\"DNN Alignment\")\n",
    "ax[0,0].set_ylabel(\"Phonemes\")\n",
    "ax[0,0].set_xlabel(\"Time\")\n",
    "f.colorbar(im, ax=ax[0,0])\n",
    "\n",
    "im = ax[0,1].imshow(np.log(dnn_align), aspect='auto', interpolation='nearest')\n",
    "ax[0,1].set_title(\"DNN Alignment in log scale\")\n",
    "ax[0,1].set_ylabel(\"Phonemes\")\n",
    "ax[0,1].set_xlabel(\"Time\")\n",
    "f.colorbar(im, ax=ax[0,1])\n",
    "\n",
    "im = ax[1,0].imshow(dnn_align, aspect='auto', interpolation='nearest')\n",
    "ax[1,0].set_title(\"RNN Alignment\")\n",
    "ax[1,0].set_ylabel(\"Phonemes\")\n",
    "ax[1,0].set_xlabel(\"Time\")\n",
    "f.colorbar(im, ax=ax[1,0])\n",
    "\n",
    "im = ax[1,1].imshow(np.log(dnn_align), aspect='auto', interpolation='nearest')\n",
    "ax[1,1].set_title(\"RNN Alignment in log scale\")\n",
    "ax[1,1].set_ylabel(\"Phonemes\")\n",
    "ax[1,1].set_xlabel(\"Time\")\n",
    "f.colorbar(im, ax=ax[1,1])\n",
    "\n",
    "im = ax[2,0].imshow(dnn_align, aspect='auto', interpolation='nearest')\n",
    "ax[2,0].set_title(\"BiRNN Alignment\")\n",
    "ax[2,0].set_ylabel(\"Phonemes\")\n",
    "ax[2,0].set_xlabel(\"Time\")\n",
    "f.colorbar(im, ax=ax[2,0])\n",
    "\n",
    "im = ax[2,1].imshow(np.log(dnn_align), aspect='auto', interpolation='nearest')\n",
    "ax[2,1].set_title(\"BiRNN Alignment in log scale\")\n",
    "ax[2,1].set_ylabel(\"Phonemes\")\n",
    "ax[2,1].set_xlabel(\"Time\")\n",
    "f.colorbar(im, ax=ax[2,1])\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "lnUmkwSbY0Ho",
    "qg4JIKs4EYtQ",
    "GLRub0wZY0Hs",
    "Zc1A7_xcwiT5",
    "3cP79coczzan",
    "hI6wj5hrz9vJ",
    "txfmuahS0Apx",
    "1nu4ISUMGt-J",
    "AOSUt5iox8B6",
    "TULzSmkZxgEn",
    "PfZPlnRjjmOJ",
    "oe_g0pXw21Tn",
    "gB79UDNM8UOZ"
   ],
   "name": "homework2_student.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.undefined.undefined"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00fad4fd55734855b6c7a6064f5f5539": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0aaae5f2d48346d0b628c190a3f303c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "22067a7eff2846d980ed8709b5ccf34d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3446a1862240433986859ff654da072e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39b18c3bb5e74345af203a673160e75a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c5f74921d7a241abb0faf420038bba2f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_00fad4fd55734855b6c7a6064f5f5539",
      "value": "100%"
     }
    },
    "3d20813547484685b82488b48047f8cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c4bfd62d26b4a028382ee2f57172d4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57612ae51b3b48c9be2fec9ffc78feae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22067a7eff2846d980ed8709b5ccf34d",
      "max": 346663984,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ebc2bb77d43e46d9be161b708f5aa72c",
      "value": 346663984
     }
    },
    "5d27efc374ef4ee194630ef570e7711a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6e704f8a8d94193bab65f6b9fa8b923",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_4c4bfd62d26b4a028382ee2f57172d4a",
      "value": "100%"
     }
    },
    "5de35340d6844d229bd8ce7aef425e29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_39b18c3bb5e74345af203a673160e75a",
       "IPY_MODEL_6ae4780ddda84918ac85b1b2df8b8d35",
       "IPY_MODEL_f270e58c76234ed9a532ddf73fef56b9"
      ],
      "layout": "IPY_MODEL_3d20813547484685b82488b48047f8cb"
     }
    },
    "6ae4780ddda84918ac85b1b2df8b8d35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3446a1862240433986859ff654da072e",
      "max": 6387309499,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fea0998bea934670936c784dd9fc39c9",
      "value": 6387309499
     }
    },
    "831bda93ff3c47f7aa656c7d74b35307": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91a64e7a24044c7cafa46a8fb7674e8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eddb4d0949074c02b2c11bada933e0dc",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0aaae5f2d48346d0b628c190a3f303c6",
      "value": " 331M/331M [00:13&lt;00:00, 28.3MB/s]"
     }
    },
    "a5175588d5a24d8ba90d699887e5b8bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5f74921d7a241abb0faf420038bba2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6e704f8a8d94193bab65f6b9fa8b923": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d859df92a88742adb725137b0105d40b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5d27efc374ef4ee194630ef570e7711a",
       "IPY_MODEL_57612ae51b3b48c9be2fec9ffc78feae",
       "IPY_MODEL_91a64e7a24044c7cafa46a8fb7674e8d"
      ],
      "layout": "IPY_MODEL_831bda93ff3c47f7aa656c7d74b35307"
     }
    },
    "ebc2bb77d43e46d9be161b708f5aa72c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eddb4d0949074c02b2c11bada933e0dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f270e58c76234ed9a532ddf73fef56b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5175588d5a24d8ba90d699887e5b8bb",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_fabcad8fad7d40c5bd07debd55b0a184",
      "value": " 5.95G/5.95G [04:01&lt;00:00, 26.9MB/s]"
     }
    },
    "fabcad8fad7d40c5bd07debd55b0a184": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fea0998bea934670936c784dd9fc39c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
